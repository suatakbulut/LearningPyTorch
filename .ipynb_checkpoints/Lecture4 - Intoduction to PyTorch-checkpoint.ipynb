{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **tensor** is a generalization of matrices. A vector is 1-dim tensor, a matrix is a 2-dim tensor, and RGB color image is an array with three indices, so it is 3-dim tensor. \n",
    "\n",
    "1. [Single Layer Network](#single_layer_network)\n",
    "2. [Multi-Layer Network](#multi_layer_network)\n",
    "3. [Numpy to Torch and Back](#numpy_to_torch_and_back)\n",
    "4. [Building networks with PyTorch](#building_networks_with_pytorch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firts, let's import PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    '''Sigmoid activation fnc\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: torch.Tensor\n",
    "    '''\n",
    "    return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='single_layer_network'></a>\n",
    "## Single Layer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some fake data \n",
    "torch.manual_seed(3) # set the random seed \n",
    "\n",
    "# tensors from a normal distribution 1row 5 cols\n",
    "features = torch.randn((1,5))\n",
    "\n",
    "# some weigths\n",
    "weights = torch.randn_like(features)\n",
    "bias = torch.randn((1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us calculate the output of this single layered network. \n",
    "\n",
    "$$ y = f \\left( \\sum_i w_i X_i + b \\right) $$\n",
    "where f is the activation function, w is weights, and X is the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6220]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = activation(torch.sum(features * weights) + bias)\n",
    "# y = activation((features * weights).sum() + bias)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most efficient way to multiply tensors is using \n",
    "   1. **torch.mm** : prefer this one. More strict on the dimensions, if there is a problem yields an error \n",
    "   2. **torch.matmul** : supports broadcasting. tensors with weird shape might yield an output. \n",
    "\n",
    "You will check your tensors' shape a lot using **tensor.shape**. Reshaping methods are:\n",
    "   1. **weights.reshape(a,b)** : the actual data in the memory will not be change, however sometimes instead it changes the original data \n",
    "   2. **weights.resize_(a,b)** : if you request more or less data than the original data your original data will loose some data \n",
    "   3. **weights.view(a,b)** : *TEHE BEST ONE.* returns (a,b) shape of the tensor without messing with the original one\n",
    "   \n",
    "Now, let s do the same calculation using matrix multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we must reshape weights from (1,5) to (5,1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6220]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = activation( torch.mm(features, weights.view(5,1)) + bias )\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='multi_layer_network'></a>\n",
    "## Multi-Layer Network\n",
    "\n",
    "Now suppose we have n input layers and a hidden layer with 2 neurons, $h_1$ and $h_2$. \n",
    "We can find the output $h_1$ as \n",
    "$$ h_1 = f \\left( \\sum_i x_i w_{1i} + bias_1 \\right) $$\n",
    "Similarly, \n",
    "$$ h_2 = f \\left( \\sum_i x_i w_{2i} + bias_2 \\right) $$\n",
    "Or as a matrix multiplication, we can do the following \n",
    "\n",
    "$$ \n",
    "\\vec{h} =  [h_1 h_2] = f \\left( [x_1 x_2 \\ldots x_n] \\cdot \\begin{bmatrix}\n",
    "           w_{11}  & w_{21}\\\\\n",
    "           w_{22}  & w_{22}\\\\\n",
    "           \\vdots  & \\vdots \\\\\n",
    "           w_{n1}  & w_{n2}\n",
    "         \\end{bmatrix} + Bias \\right)\n",
    "$$\n",
    "\n",
    "Now let's create some fake data and calculate the output of a multi layer network with one hidden layer. \n",
    "\n",
    "<img src = \"multilayer_network.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some fake data \n",
    "torch.manual_seed(3) # set the random seed \n",
    "\n",
    "# Create 3 features \n",
    "features = torch.randn((1,3)) \n",
    "\n",
    "# Number of neurons in layers: input, hidden, and output\n",
    "n_input = features.shape[1] \n",
    "n_hidden = 2 \n",
    "n_ouput = 2\n",
    "\n",
    "# From input to hidden layer n_input x n_hidden\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "\n",
    "# From hidden layer to output n_hidden x n_ouput\n",
    "W2 = torch.randn(n_hidden, n_ouput) \n",
    "B2 = torch.randn((1, n_ouput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features: torch.Size([1, 3])\n",
      "Shape of W1: torch.Size([3, 2])\n",
      "Shape of W2: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "print('Shape of features: {}'.format(features.shape))\n",
    "print('Shape of W1: {}'.format(W1.shape))\n",
    "print('Shape of W2: {}'.format(W2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6958, 0.6643]])\n"
     ]
    }
   ],
   "source": [
    "h = activation(torch.mm(features, W1) + B1)\n",
    "output = activation(torch.mm(h, W2) + B2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of hidden units parameters of the network is hyperparameter.\n",
    "<a id='numpy_to_torch_and_back'></a>\n",
    "## Numpy to Torch and Back \n",
    "\n",
    "To convert to a numpy array to a tensor **torch.from_numpy()**\n",
    "From a tensor to a numpy **.numpy()** \n",
    "\n",
    "The memory is shared between Numpy and Torch. So if you change one value the other one will be changed as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8157,  0.4952, -0.1643, -0.6780, -1.0591],\n",
      "        [ 0.7477,  0.2389, -0.3922,  0.1519, -1.1837],\n",
      "        [ 0.5344, -1.4510, -0.6294,  0.1544, -0.2480]])\n",
      "[[ 0.8156775   0.495189   -0.16431698 -0.6779622  -1.0591074 ]\n",
      " [ 0.74769664  0.23891741 -0.39215022  0.1519149  -1.1837332 ]\n",
      " [ 0.5343607  -1.4510227  -0.629374    0.1544151  -0.24799016]]\n",
      "tensor([[ 0.8157,  0.4952, -0.1643, -0.6780, -1.0591],\n",
      "        [ 0.7477,  0.2389, -0.3922,  0.1519, -1.1837],\n",
      "        [ 0.5344, -1.4510, -0.6294,  0.1544, -0.2480]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randn((3,5))\n",
    "print(tensor) \n",
    "numpy = tensor.numpy()\n",
    "print(numpy)\n",
    "tensor_back = torch.from_numpy(numpy)\n",
    "print(tensor_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='neural_networks_with_pytorch'></a>\n",
    "## Neural Networks with PyTorch \n",
    "\n",
    "PyTorch has a nice module **nn** that provides an efficient way of building large neural networks. \n",
    "\n",
    "To do that we will use MNIST image dataset, which consists of grayscale handwritten digits, we will try to predict. Each image is 28x28 pixels. \n",
    "\n",
    "First off, let's load the dataset through torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some necessary packages \n",
    "%matplotlib inline\n",
    "\n",
    "# import torch (we have imported it at the beginning)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms # conda install torchvision -c soumith to install torchvision (Windows, conda) \n",
    "# Define a transofmration to normalize data \n",
    "transform = transforms.Compose ([transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))\n",
    "                                ])\n",
    "\n",
    "# Download and load the train data\n",
    "train_set = datasets.MNIST('MNIST_data/train/', download = True, train = True, transform = transform) \n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size = 64, shuffle = True)\n",
    "\n",
    "# Download and load the test data\n",
    "test_set = datasets.MNIST('MNIST_data/test/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data into generators, trainloader and testloader. We will turn them into iterator. Or we can use for loop and loop through the generator. Since we chose **batch_size=64**, the images below are just a tensor with size **(64, 1, 28, 28)**. In other words, we have 64 of 1 color channel and 28x28 pixel images images per batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images,  labels = dataiter.next() \n",
    "\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does an image look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image below is labeled as a \"2\".\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap = 'Greys_r'); \n",
    "print('The image below is labeled as a \"{}\".'.format(labels[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build a neural network for this dataset using weight matrices and matrix multiplication. Then we will do it with PyTorch's **nn** module. \n",
    "\n",
    "For this exersice we will build a *Dense* (*fully-connected*) network. In a dense network, wach unit in one layer is connected to each unit in the next layer. In fully-connected networks, the input to each layer must be a one-dimensional vector. However, our images are 28x28 2D tensors, so we need to convert them into 1D vectors. Thinking about sizes, we need to convert the batch of images with shape (64, 1, 28, 28) to a have a shape of (64, 784), where 28x28 = 784. This is typically called flattening, we flattened the 2D images into 1D vectors.\n",
    "\n",
    "Moreover, unlike above where we had only one output unit in the output layer, here we need 10 output units, one for each digit. We want our network to predict the digit shown in an image, so what we'll do is calculate probabilities that the image is of any one digit or class. This ends up being a discrete probability distribution over the classes (digits) that tells us the most likely class for the image. That means we need 10 output units for the 10 classes (digits). \n",
    "\n",
    "    Exercise: Flatten the batch of images images. Then build a multi-layer network with 784 input units, 256 hidden units, and 10 output units using random tensors for the weights and biases. For now, use a sigmoid activation for the hidden layer. Leave the output layer without an activation, we'll add one that gives us a probability distribution next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the activation function (for now stick to sigmoid activation function)\n",
    "\n",
    "def activation(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "# flatten the input\n",
    "inputs = images.view(images.shape[0], -1) \n",
    "# -1 handles the remaning part so we do not need to calculate it. Here we preserve \n",
    "# 64, the number of the observations and flatten the (1,28,28) part to (784) \n",
    "\n",
    "# create weights and biases\n",
    "# from input to hidden layer\n",
    "w1 = torch.randn(784,256)\n",
    "b1 = torch.randn(256)\n",
    "\n",
    "# from hidden to output \n",
    "w2 = torch.randn(256, 10)\n",
    "b2 = torch.randn(10) \n",
    "\n",
    "h = activation( torch.mm(inputs, w1) + b1 ) # the outputs of the neurons in the hidden layer \n",
    "\n",
    "output = torch.mm(h, w2) + b2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do want our network to tell us the probabilities of an image belonging to a class of digit. In other words, we want it to result in a probability distribution. However, this one yields the following "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2112e+01,  1.2777e+00,  4.9910e+00, -3.4567e+00, -6.6953e+00,\n",
       "         -7.4234e+00,  2.2651e+01, -3.2819e+00,  2.3959e+01, -3.4915e+00],\n",
       "        [-1.4391e+01,  1.7829e+00,  4.9559e+00,  1.0844e+01, -1.0100e+01,\n",
       "         -2.2191e+00,  4.7274e+00,  1.2272e+01,  2.8499e+01, -7.3339e+00],\n",
       "        [-1.4391e+01,  1.1616e+01,  2.3482e+00,  8.6546e+00,  2.2053e+00,\n",
       "         -4.0691e+00,  6.2809e+00,  6.7545e+00,  1.9164e+01, -8.3233e+00],\n",
       "        [-1.6442e+01,  1.0186e+01,  5.2337e+00,  1.3225e+01, -2.1392e+01,\n",
       "         -8.1322e+00,  1.2296e+01,  1.0321e+01,  3.1405e+01, -2.0495e-01],\n",
       "        [-1.3077e+01,  6.1945e+00,  7.8312e+00, -8.1477e-01, -3.6199e+00,\n",
       "         -1.3158e+01,  1.4472e+01,  1.2811e+01,  2.3580e+01,  7.4187e+00],\n",
       "        [-4.5942e+00,  3.0173e+00,  7.4091e+00,  4.1353e+00, -7.9112e+00,\n",
       "         -9.5364e+00,  1.5518e+01,  6.1251e+00,  2.3707e+01, -4.3137e+00],\n",
       "        [-1.8694e+01,  1.0105e+00, -3.0194e+00, -4.1152e-01, -1.0928e+01,\n",
       "         -3.6953e+00,  1.0125e+01,  1.3416e+01,  2.0074e+01, -4.7075e+00],\n",
       "        [-1.4936e+01,  7.8783e+00,  1.9744e-01,  1.2738e+01, -9.8776e+00,\n",
       "         -1.1266e+00,  7.9883e+00,  9.1121e+00,  1.8125e+01, -1.1388e+01],\n",
       "        [-1.0344e+01,  8.1002e+00,  2.9137e+00,  4.8156e+00, -1.4819e+01,\n",
       "         -6.0080e+00,  8.3242e+00,  1.8189e+00,  3.3744e+01, -1.1600e+01],\n",
       "        [-1.9986e+01,  3.2111e-01,  1.2227e+00,  4.1178e+00, -6.9024e+00,\n",
       "         -6.1170e+00,  1.4870e+01,  1.6031e+00,  1.3791e+01,  2.4732e+00],\n",
       "        [-9.1347e+00,  2.3159e+00, -4.0607e+00, -8.4601e+00, -1.1215e+01,\n",
       "         -3.2539e+00,  1.6909e+01,  1.1167e+01,  2.0065e+01, -3.2711e+00],\n",
       "        [-6.9571e+00, -1.8381e+00,  1.0622e+00, -2.6446e+00, -2.1027e+01,\n",
       "         -9.1902e+00,  8.5856e+00,  1.3314e+01,  2.9452e+01, -2.2454e+00],\n",
       "        [-1.0093e+01,  1.3426e+01,  1.4116e+00,  4.2510e+00, -1.4003e+01,\n",
       "          3.0269e+00,  2.5042e+01,  5.7405e+00,  2.3010e+01, -3.5809e+00],\n",
       "        [-1.1568e+01, -3.5155e-01, -4.2043e+00,  3.1991e+00, -1.8584e+01,\n",
       "         -2.0041e+01,  1.9611e+01,  3.7246e+00,  2.5657e+01, -1.1257e+00],\n",
       "        [-1.7207e+01,  6.0937e+00,  6.1411e+00, -5.5068e+00, -1.2421e+01,\n",
       "         -9.1436e-01,  1.4391e+01,  4.2945e+00,  2.5827e+01, -2.7324e+00],\n",
       "        [-6.1304e+00,  7.4227e+00,  7.2324e+00, -1.5037e+00, -1.3454e+01,\n",
       "         -5.7936e+00,  1.1835e+01,  4.0822e+00,  2.5258e+01, -5.2027e-01],\n",
       "        [-1.7917e+01,  8.6895e+00,  3.0823e+00, -6.4215e+00, -5.2339e+00,\n",
       "         -3.8184e+00,  7.0090e+00,  2.8607e-01,  2.6657e+01,  8.9057e+00],\n",
       "        [-1.1660e+01,  8.3572e+00,  5.9443e+00,  4.9050e+00, -1.8579e+01,\n",
       "         -1.7385e+01, -2.0307e-01,  4.3071e-01,  2.4905e+01,  3.7666e+00],\n",
       "        [-1.2454e+01,  1.1114e+01,  4.3374e+00, -2.6272e+00, -6.8769e+00,\n",
       "          4.8598e-01,  1.7077e+01,  5.3054e+00,  2.5281e+01,  2.7100e-01],\n",
       "        [-7.1826e+00,  5.4819e+00, -3.3939e+00,  3.2188e+00, -2.8717e+01,\n",
       "         -8.1862e+00,  1.4254e+01,  1.1167e+01,  2.8566e+01, -2.0390e+00],\n",
       "        [-1.3296e+01,  9.3885e+00, -4.6092e+00, -8.5072e-01, -1.5391e+01,\n",
       "         -1.3576e+01,  1.9756e+01,  1.4772e+01,  2.9816e+01,  1.5467e+00],\n",
       "        [-1.4937e+01, -7.6299e-01, -3.9097e-01, -9.8026e+00, -1.8034e+01,\n",
       "         -1.7563e+01,  2.2376e+01,  1.7516e+00,  1.6756e+01,  3.7862e+00],\n",
       "        [-1.7016e+01,  7.2086e+00,  7.0598e+00,  7.5511e+00, -2.5798e+01,\n",
       "          3.2035e+00,  2.0131e+01,  6.6277e+00,  1.6228e+01, -7.2165e-01],\n",
       "        [-1.3992e+01,  6.4494e+00, -3.9188e+00,  4.2488e+00, -8.9505e+00,\n",
       "         -2.3824e+00,  1.8288e+01,  4.0639e+00,  2.0772e+01, -1.0956e+01],\n",
       "        [-1.3970e+01,  6.8285e+00,  2.8514e-01,  7.5775e+00, -1.0412e+01,\n",
       "         -1.4846e+01,  1.7017e+01,  1.8496e+01,  1.0702e+01,  5.4471e-01],\n",
       "        [-2.4295e+01,  9.2952e+00, -4.7498e+00,  5.4521e+00, -1.9924e+01,\n",
       "         -2.8242e+00,  1.6013e+01,  1.2480e+01,  2.7091e+01,  4.7415e-01],\n",
       "        [-1.3105e+01,  7.4563e+00,  1.2002e+01,  9.9018e+00, -6.0930e+00,\n",
       "         -6.0516e+00,  1.8624e+01,  2.2278e+00,  2.5243e+01, -1.2773e+00],\n",
       "        [-1.5281e+01,  1.2589e+00,  2.2519e-01, -4.6980e+00, -7.1075e-01,\n",
       "         -8.2812e+00,  1.5184e+01,  5.2565e+00,  1.6782e+01,  3.8448e+00],\n",
       "        [-5.8858e-01,  3.1971e+00, -5.8418e+00,  4.3814e+00, -4.1535e+00,\n",
       "         -1.1563e+01,  1.9284e+01,  8.7951e+00,  1.6563e+01, -4.4623e+00],\n",
       "        [-1.4909e+01, -1.8114e-01,  3.6887e+00, -2.9617e+00, -1.1718e+01,\n",
       "          9.9034e+00,  1.2660e+01,  1.2937e+01,  3.0189e+01, -7.5907e+00],\n",
       "        [-8.2794e+00,  6.4378e+00,  2.3578e+00, -4.4809e-01, -1.6795e+01,\n",
       "         -1.0242e+01,  1.5785e+01,  6.5902e+00,  1.7771e+01,  4.5217e+00],\n",
       "        [-1.2251e+01, -1.8014e+00,  4.1647e+00,  2.2252e+00, -1.8321e+01,\n",
       "         -5.8887e+00,  1.1044e+01,  7.8594e+00,  2.4988e+01,  4.1802e-01],\n",
       "        [-1.5262e+01,  9.3922e+00,  6.2112e+00,  2.5564e+00, -1.1218e+01,\n",
       "         -4.7745e+00,  1.4646e+01,  8.6456e+00,  2.4674e+01,  4.3549e-01],\n",
       "        [-8.5702e+00,  2.6002e+00,  6.7537e+00, -1.6310e+00, -1.1840e+01,\n",
       "         -7.0464e+00,  1.5838e+01,  1.2747e+00,  2.9866e+01,  5.4405e-01],\n",
       "        [-2.2510e+01,  1.3127e+01, -1.3670e+00, -1.5610e+00, -1.4123e+01,\n",
       "         -5.0118e+00,  1.2935e+01,  1.0275e+01,  2.6958e+01, -1.1127e+00],\n",
       "        [-1.3602e+01,  1.4050e+01,  2.1165e+00, -3.1218e+00, -2.1479e+00,\n",
       "          1.1979e+00,  6.8772e+00,  1.0743e+01,  1.3140e+01, -1.6990e+00],\n",
       "        [-1.0077e+01,  1.0260e+00,  4.3657e+00, -6.4537e-01, -2.0914e+01,\n",
       "         -9.9247e+00,  2.1098e+01,  8.1002e+00,  2.2992e+01, -7.0745e+00],\n",
       "        [-4.1822e+00,  1.1570e+01, -2.6843e+00, -5.0312e+00, -1.2459e+01,\n",
       "         -3.0595e+00,  1.3074e+01,  1.0882e+01,  1.1486e+01,  2.7602e+00],\n",
       "        [-8.2333e+00,  9.3065e-01,  8.5549e+00,  7.7205e+00, -7.3891e+00,\n",
       "         -9.7967e+00,  2.9393e+01,  7.9172e-01,  1.9531e+01, -1.9508e+00],\n",
       "        [-1.5615e+01,  2.8032e+00,  3.7542e+00,  4.9866e+00, -9.1807e+00,\n",
       "         -1.3279e+00,  1.4838e+01,  6.6118e+00,  1.0778e+01, -4.1917e+00],\n",
       "        [-1.3662e+01,  1.2271e+00,  6.6350e+00,  3.0948e+00, -1.3624e+01,\n",
       "         -7.4507e+00,  1.7740e+01,  6.6405e+00,  1.7740e+01, -4.3981e+00],\n",
       "        [-6.6760e+00,  1.7482e+00,  3.5098e+00,  8.3424e+00, -1.8496e+01,\n",
       "         -6.3995e+00,  1.5988e+01,  9.0914e+00,  1.9397e+01,  4.7696e+00],\n",
       "        [-8.6700e+00,  1.0648e+01, -2.8793e+00, -3.1533e+00, -9.5070e+00,\n",
       "         -4.3440e+00,  2.0116e+01,  4.1586e+00,  2.8331e+01,  1.4960e+00],\n",
       "        [-1.5838e+01,  6.4647e+00, -3.2342e+00,  4.9986e+00, -1.1453e+01,\n",
       "         -7.8224e+00,  1.5577e+01,  1.2529e+01,  1.7447e+01,  1.5893e+00],\n",
       "        [-1.7143e+01,  5.0277e+00,  5.0736e+00,  1.4459e+01, -2.0756e+01,\n",
       "         -7.6425e+00,  1.6681e+01,  8.9757e+00,  2.5058e+01, -3.1389e+00],\n",
       "        [-1.7589e+01,  4.5763e+00,  1.7357e+01,  2.7479e+00, -1.7157e+01,\n",
       "         -6.4231e+00,  1.4093e+01,  2.4391e+00,  2.1706e+01,  7.4397e-01],\n",
       "        [-2.5634e+01,  2.4707e+00,  6.6963e+00,  3.9047e+00, -1.5476e+01,\n",
       "         -1.5098e+01,  8.5684e+00,  7.9882e+00,  2.2305e+01,  5.2065e-01],\n",
       "        [-9.0533e+00,  5.4872e+00,  6.8792e+00,  1.0572e+01, -1.0507e+01,\n",
       "         -1.8973e+01,  2.3907e+00,  1.2720e+01,  1.8936e+01,  8.1601e-01],\n",
       "        [-1.5796e+01,  9.0347e+00,  1.6417e+00, -3.7354e+00, -8.1314e+00,\n",
       "         -5.3035e-01,  1.7107e+01,  4.4388e+00,  2.6372e+01, -2.8264e-01],\n",
       "        [-1.2896e+01,  7.0648e+00, -6.7918e+00,  8.8907e+00, -1.9602e+01,\n",
       "          4.9141e+00,  1.4596e+01,  7.8403e+00,  1.6347e+01, -4.2809e+00],\n",
       "        [-1.2993e+01, -6.7764e+00,  1.0072e+01,  2.4731e-02, -5.8459e+00,\n",
       "         -5.5864e+00,  6.4296e-01,  1.4722e+01,  1.2529e+01,  8.0141e+00],\n",
       "        [-1.3812e+01, -3.7788e+00, -4.9206e+00,  3.4319e+00,  5.0907e-01,\n",
       "         -1.1859e+01,  1.6165e+01, -2.9262e+00,  1.1267e+01,  5.4080e+00],\n",
       "        [-1.0815e+01,  2.4211e+00,  3.1776e+00, -1.3155e+00, -1.0929e+01,\n",
       "         -8.3358e+00,  2.2264e+01,  2.3248e+00,  2.3301e+01, -1.2418e+00],\n",
       "        [-7.9342e+00, -5.3025e-01,  2.9311e+00, -3.1304e+00, -2.1400e+01,\n",
       "         -5.5878e+00,  2.1218e+00,  1.7025e+01,  3.1819e+01,  8.6629e-01],\n",
       "        [-1.6260e+01,  7.1877e+00, -1.1738e+00,  1.0757e+01, -4.1391e+00,\n",
       "         -2.2822e+00,  3.7933e+00,  8.2035e+00,  2.0724e+01, -9.3450e+00],\n",
       "        [-5.1703e+00, -1.7711e+00, -7.1464e-01,  1.7843e-01, -1.3067e+01,\n",
       "         -9.1064e+00,  1.1077e+01,  2.1033e+00,  1.3686e+01, -4.2781e+00],\n",
       "        [-9.6988e+00,  8.6364e+00,  2.8316e+00,  1.5142e+01, -3.7984e+00,\n",
       "         -5.5728e+00,  1.0455e+01,  1.1105e+01,  2.1946e+01,  3.2206e+00],\n",
       "        [-1.3857e+01,  1.1946e+01,  3.3778e+00,  4.4827e+00, -1.1327e+01,\n",
       "         -1.5681e+01,  1.0354e+01,  1.8255e+01,  1.6712e+01,  5.7978e+00],\n",
       "        [-1.1183e+01,  7.7734e-01,  2.7514e+00, -2.9643e+00, -7.0961e-01,\n",
       "         -1.1687e+00,  1.8735e+01,  1.6757e+00,  2.5432e+00,  2.1977e+00],\n",
       "        [-5.5840e+00,  1.6808e+00,  3.2297e-01,  1.9220e+00, -1.3180e+01,\n",
       "         -1.3426e+01,  1.6934e+01,  1.1589e+01,  2.1923e+01, -8.2798e+00],\n",
       "        [-1.1408e+01,  8.1133e+00, -1.3237e+00,  6.4785e+00, -5.4832e+00,\n",
       "         -3.9905e+00,  2.0072e+01,  8.6053e+00,  1.8388e+01,  3.1412e+00],\n",
       "        [-1.6390e+01, -2.4640e+00,  1.6498e+00, -1.6290e+00, -1.1673e+01,\n",
       "         -8.8026e+00,  1.2240e+01,  7.6745e+00,  2.3416e+01, -1.1095e+00],\n",
       "        [-8.6491e+00,  1.3672e+01,  1.1130e+01,  1.3544e+00, -1.8955e+01,\n",
       "         -9.0859e+00,  1.0716e+01,  4.3580e+00,  1.9234e+01,  4.0885e-01],\n",
       "        [-2.2103e+01, -2.0001e+00,  4.1477e+00,  5.5062e+00, -9.7319e+00,\n",
       "         -1.3704e+01,  1.2559e+01,  7.5533e+00,  2.3328e+01, -5.7678e+00]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we want to convert these values into a probability distribution. For this we use [softmax](https://en.wikipedia.org/wiki/Softmax_function) function.  It looks like\n",
    "$$ \\sigma(x_i) = \\frac{e^{x_i}}{\\sum_{k} e^{x_k}}$$ \n",
    "\n",
    "    Exercise: Implement a function softmax that performs the softmax calculation and returns probability distributions for each example in the batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    '''\n",
    "    Note that you'll need to pay attention to the shapes when doing this. If you have a tensor a with \n",
    "    shape (64, 10) and a tensor b with shape (64,), doing a/b will give you an error because PyTorch \n",
    "    will try to do the division across the columns (called broadcasting) but you'll get a size mismatch. \n",
    "    The way to think about this is for each of the 64 examples, you only want to divide by one value, \n",
    "    the sum in the denominator. So you need b to have a shape of (64, 1). This way PyTorch will divide \n",
    "    the 10 values in each row of a by the one value in each row of b. Pay attention to how you take \n",
    "    the sum as well. You'll need to define the dim keyword in torch.sum. Setting dim=0 takes the sum \n",
    "    across the rows while dim=1 takes the sum across the columns.\n",
    "    '''\n",
    "    return torch.exp(x) / torch.sum(torch.exp(x), dim=1).view(-1, 1)\n",
    "\n",
    "output_probs = softmax(output) \n",
    "# Check that it has the sahpe (64, 10)\n",
    "print(output_probs.shape)\n",
    "# Check that the probs add up to 1\n",
    "print(output_probs.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='building_networks_with_pytorch'></a>\n",
    "## Building networks with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<a id='new_title'></a>\n",
    "## New Title"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
