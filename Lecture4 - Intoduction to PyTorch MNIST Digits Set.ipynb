{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center>PyTorch</center> </h1>\n",
    "\n",
    "A **tensor** is a generalization of matrices. A vector is 1-dim tensor, a matrix is a 2-dim tensor, and RGB color image is an array with three indices, so it is 3-dim tensor. \n",
    "\n",
    "Table of Contents\n",
    "1. [Single Layer Network](#single_layer_network) \n",
    "1. [Multi-Layer Network](#multi_layer_network)\n",
    "1. [Numpy to Torch and Back](#numpy_to_torch_and_back) \n",
    "1. [Neural Networks with PyTorch](#neural_networks_with_pytorch)\n",
    "1. [Building networks with PyTorch from scratch](#building_networks_with_pytorch) \n",
    "    1. [Activation Functions](#activation_functions)  \n",
    "    1. [Initializing weights and biases](#initializing_weights_and_biases)  \n",
    "    1. [Forward Pass](#forward_pass)\n",
    "1. [Building Networks using `nn.Sequential`](#sequential)\n",
    "1. [Model Training](#training)\n",
    "    1. [Autograd](#autograd)\n",
    "    1. [Loss and Autograd Together ](#loss_autograd)\n",
    "    1. [Training the network](#training_network)\n",
    "1. [Training For Real](#training_forreal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firts, let's import PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    '''Sigmoid activation fnc\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: torch.Tensor\n",
    "    '''\n",
    "    return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='single_layer_network'></a>\n",
    "## 1. Single Layer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some fake data \n",
    "torch.manual_seed(3) # set the random seed \n",
    "\n",
    "# tensors from a normal distribution 1row 5 cols\n",
    "features = torch.randn((1,5))\n",
    "\n",
    "# some weigths\n",
    "weights = torch.randn_like(features)\n",
    "bias = torch.randn((1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us calculate the output of this single layered network. \n",
    "\n",
    "$$ y = f \\left( \\sum_i w_i X_i + b \\right) $$\n",
    "where f is the activation function, w is weights, and X is the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6220]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = activation(torch.sum(features * weights) + bias)\n",
    "# y = activation((features * weights).sum() + bias)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most efficient way to multiply tensors is using \n",
    "   1. **torch.mm** : prefer this one. More strict on the dimensions, if there is a problem yields an error \n",
    "   2. **torch.matmul** : supports broadcasting. tensors with weird shape might yield an output. \n",
    "\n",
    "You will check your tensors' shape a lot using **tensor.shape**. Reshaping methods are:\n",
    "   1. **weights.reshape(a,b)** : the actual data in the memory will not be change, however sometimes instead it changes the original data \n",
    "   2. **weights.resize_(a,b)** : if you request more or less data than the original data your original data will loose some data \n",
    "   3. **weights.view(a,b)** : *TEHE BEST ONE.* returns (a,b) shape of the tensor without messing with the original one\n",
    "   \n",
    "Now, let s do the same calculation using matrix multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we must reshape weights from (1,5) to (5,1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6220]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = activation( torch.mm(features, weights.view(5,1)) + bias )\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='multi_layer_network'></a>\n",
    "## 2. Multi-Layer Network\n",
    "\n",
    "Now suppose we have n input layers and a hidden layer with 2 neurons, $h_1$ and $h_2$. \n",
    "We can find the output $h_1$ as \n",
    "$$ h_1 = f \\left( \\sum_i x_i w_{1i} + bias_1 \\right) $$\n",
    "Similarly, \n",
    "$$ h_2 = f \\left( \\sum_i x_i w_{2i} + bias_2 \\right) $$\n",
    "Or as a matrix multiplication, we can do the following \n",
    "\n",
    "$$ \n",
    "\\vec{h} =  [h_1 h_2] = f \\left( [x_1 x_2 \\ldots x_n] \\cdot \\begin{bmatrix}\n",
    "           w_{11}  & w_{21}\\\\\n",
    "           w_{22}  & w_{22}\\\\\n",
    "           \\vdots  & \\vdots \\\\\n",
    "           w_{n1}  & w_{n2}\n",
    "         \\end{bmatrix} + Bias \\right)\n",
    "$$\n",
    "\n",
    "Now let's create some fake data and calculate the output of a multi layer network with one hidden layer. \n",
    "\n",
    "<img src = \"multilayer_network.png\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some fake data \n",
    "torch.manual_seed(3) # set the random seed \n",
    "\n",
    "# Create 3 features \n",
    "features = torch.randn((1,3)) \n",
    "\n",
    "# Number of neurons in layers: input, hidden, and output\n",
    "n_input = features.shape[1] \n",
    "n_hidden = 2 \n",
    "n_ouput = 2\n",
    "\n",
    "# From input to hidden layer n_input x n_hidden\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "\n",
    "# From hidden layer to output n_hidden x n_ouput\n",
    "W2 = torch.randn(n_hidden, n_ouput) \n",
    "B2 = torch.randn((1, n_ouput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features: torch.Size([1, 3])\n",
      "Shape of W1: torch.Size([3, 2])\n",
      "Shape of W2: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "print('Shape of features: {}'.format(features.shape))\n",
    "print('Shape of W1: {}'.format(W1.shape))\n",
    "print('Shape of W2: {}'.format(W2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6958, 0.6643]])\n"
     ]
    }
   ],
   "source": [
    "h = activation(torch.mm(features, W1) + B1)\n",
    "output = activation(torch.mm(h, W2) + B2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of hidden units parameters of the network is hyperparameter.\n",
    "<a id='numpy_to_torch_and_back'></a>\n",
    "## 3. Numpy to Torch and Back \n",
    "\n",
    "To convert to a numpy array to a tensor **torch.from_numpy()**\n",
    "From a tensor to a numpy **.numpy()** \n",
    "\n",
    "The memory is shared between Numpy and Torch. So if you change one value the other one will be changed as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8157,  0.4952, -0.1643, -0.6780, -1.0591],\n",
      "        [ 0.7477,  0.2389, -0.3922,  0.1519, -1.1837],\n",
      "        [ 0.5344, -1.4510, -0.6294,  0.1544, -0.2480]])\n",
      "[[ 0.8156775   0.495189   -0.16431698 -0.6779622  -1.0591074 ]\n",
      " [ 0.74769664  0.23891741 -0.39215022  0.1519149  -1.1837332 ]\n",
      " [ 0.5343607  -1.4510227  -0.629374    0.1544151  -0.24799016]]\n",
      "tensor([[ 0.8157,  0.4952, -0.1643, -0.6780, -1.0591],\n",
      "        [ 0.7477,  0.2389, -0.3922,  0.1519, -1.1837],\n",
      "        [ 0.5344, -1.4510, -0.6294,  0.1544, -0.2480]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randn((3,5))\n",
    "print(tensor) \n",
    "numpy = tensor.numpy()\n",
    "print(numpy)\n",
    "tensor_back = torch.from_numpy(numpy)\n",
    "print(tensor_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='neural_networks_with_pytorch'></a>\n",
    "## 4. Neural Networks with PyTorch \n",
    "\n",
    "PyTorch has a nice module **nn** that provides an efficient way of building large neural networks. \n",
    "\n",
    "To do that we will use MNIST image dataset, which consists of grayscale handwritten digits, we will try to predict. Each image is 28x28 pixels. \n",
    "\n",
    "First off, let's load the dataset through torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some necessary packages \n",
    "%matplotlib inline\n",
    "\n",
    "# import torch (we have imported it at the beginning)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms # conda install torchvision -c soumith to install torchvision (Windows, conda) \n",
    "# Define a transofmration to normalize data \n",
    "transform = transforms.Compose ([transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))\n",
    "                                ])\n",
    "\n",
    "# Download and load the train data\n",
    "train_set = datasets.MNIST('MNIST_data/train/', download = True, train = True, transform = transform) \n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size = 64, shuffle = True)\n",
    "\n",
    "# Download and load the test data\n",
    "test_set = datasets.MNIST('MNIST_data/test/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data into generators, trainloader and testloader. We will turn them into iterator. Or we can use for loop and loop through the generator. Since we chose **batch_size=64**, the images below are just a tensor with size **(64, 1, 28, 28)**. In other words, we have 64 of 1 color channel and 28x28 pixel images images per batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images,  labels = dataiter.next() \n",
    "\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does an image look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image below is labeled as a \"2\".\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAANUklEQVR4nO3dfchc9ZnG8evSTXxr/9CVxJC62pQIuyxqliCKRbrUFleRJIQsjbJm2UIqNNDCohu6QmNCtay2iv9Un6I0uyQ2RdMa6kIqIVl3I4TElzWx2cYYsnnlCSaEWt9qzL1/PCfLY5z5zePMmTmT3N8PDDNz7ufMuTnkyjlzXubniBCAs985TTcAYDAIO5AEYQeSIOxAEoQdSOJPBrkw2xz6B/osItxqek9bdtu32P6d7d22l/byWQD6y92eZ7d9rqRdkr4m6YCkrZIWRsRvC/OwZQf6rB9b9usk7Y6IPRHxR0k/lzSnh88D0Ee9hH26pP3j3h+opn2C7cW2t9ne1sOyAPSolwN0rXYVPrWbHhEjkkYkduOBJvWyZT8g6fJx778g6VBv7QDol17CvlXSTNtftD1Z0jckraunLQB163o3PiJO2F4iab2kcyU9FRFv1NYZgFp1feqtq4XxnR3ou75cVAPgzEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBIDHbIZg7dixYpi/eabby7WZ82aVaxPnjy5WLdb/tCpJGnjxo3FeW+//fZi/d133y3W8Uls2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCUZxPQN0Otd99913t63dddddxXknTZpUrB88eLBY37NnT7F+/fXXt62dd955xXmXL19erC9btqxYz6rdKK49XVRje6+kdyR9LOlERMzu5fMA9E8dV9D9dUS8XcPnAOgjvrMDSfQa9pD0G9sv217c6g9sL7a9zfa2HpcFoAe97sbfGBGHbE+R9ILt/4mIF8f/QUSMSBqROEAHNKmnLXtEHKqej0j6paTr6mgKQP26Drvti2x//tRrSV+XtKOuxgDUq+vz7LZnaGxrLo19HVgdET/oMA+78S3cdNNNxfqaNWuK9alTp7at7dq1qzjvggULivXt27cX653cdtttbWtr164tzrtly5ZivdN6y6r28+wRsUfSNV13BGCgOPUGJEHYgSQIO5AEYQeSIOxAEvyU9BDodAppypQpxXrpJ5nnz59fnPf48ePFeq+ef/75trX333+/OO+HH35YdzupsWUHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQ4zz4EHn744WL91VdfLdZL57KH2dGjR4v1zZs3D6iTHNiyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASnGcfAh988EGxfqaeR5ekmTNntq1NmzZtgJ2ALTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJMF5dvTVfffd17Z24sSJ4ryrV6+uu53UOm7ZbT9l+4jtHeOmXWL7BdtvVs8X97dNAL2ayG78zyTdctq0pZI2RMRMSRuq9wCGWMewR8SLko6dNnmOpJXV65WS5tbcF4CadfudfWpEHJakiDhsu+1gZLYXS1rc5XIA1KTvB+giYkTSiCTZjn4vD0Br3Z56G7U9TZKq5yP1tQSgH7oN+zpJi6rXiyQ9V087APrFEeU9a9tPS/qKpEsljUr6vqRfSfqFpD+TtE/Sgog4/SBeq89iN/4sc/755xfrb731Vtvae++9V5y3dC882osIt5re8Tt7RCxsU/pqTx0BGCgulwWSIOxAEoQdSIKwA0kQdiAJbnFFT1asWFGsl34u+vHHH6+7HRSwZQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJDre4lrrwrjF9YxzwQUXFOsvvfRSsT5jxoy2tVmzZhXn3bNnT7GO1trd4sqWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeS4H52FN1///3F+jXXXFOsP/HEE21rnEcfLLbsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AE97Mn1+k8+aZNm4r1HTt2FOtz585tWzt69GhxXnSn6/vZbT9l+4jtHeOmLbN90PZr1ePWOpsFUL+J7Mb/TNItLaY/EhHXVo9/r7ctAHXrGPaIeFHSsQH0AqCPejlAt8T269Vu/sXt/sj2YtvbbG/rYVkAetRt2H8i6UuSrpV0WNKP2v1hRIxExOyImN3lsgDUoKuwR8RoRHwcEScl/VTSdfW2BaBuXYXd9vhxeOdJKp9/AdC4jufZbT8t6SuSLpU0Kun71ftrJYWkvZK+FRGHOy6M8+wDd+GFFxbrmzdvLtYvu+yyYv2GG24o1vfu3Vuso37tzrN3/PGKiFjYYvKTPXcEYKC4XBZIgrADSRB2IAnCDiRB2IEk+Cnps8AVV1zRtrZ69erivJ1ucV22bFmxzqm1MwdbdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgvPsZ4Hly5e3rXW6BfXRRx8t1h944IGuesLwYcsOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0lwnv0McO+99xbrd955Z9vavn37ivNu3bq1WJ83b16xfvXVVxfrH330Udvazp07i/N20ql37rX/JLbsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5BExyGba10YQza3dM899xTrDz74YLF+zjk5/88uncOXpKVLl7atPfLII3W3MzTaDdnc8V+J7cttb7S90/Ybtr9TTb/E9gu236yeL667aQD1mcgm4YSkf4yIP5d0vaRv2/4LSUslbYiImZI2VO8BDKmOYY+IwxHxSvX6HUk7JU2XNEfSyurPVkqa268mAfTuM10bb/tKSbMkbZE0NSIOS2P/Idie0maexZIW99YmgF5NOOy2PyfpWUnfjYjf2y2PAXxKRIxIGqk+gwN0QEMmdBjX9iSNBX1VRKytJo/anlbVp0k60p8WAdSh45bdY5vwJyXtjIgfjyutk7RI0g+r5+f60uEZYNKkScX6qlWrivX58+cX6xPdi2pl9+7dxfqaNWuK9ePHjxfrx44dK9afeeaZYr0X06dPL9b379/ft2WfiSayG3+jpL+TtN32a9W072ks5L+w/U1J+yQt6E+LAOrQMewR8V+S2m1avlpvOwD6JeelV0BChB1IgrADSRB2IAnCDiTBLa41eOyxx4r1JUuW9PT569evL9YfeuihtrVNmzYV5z158mQ3LWGIdX2LK4CzA2EHkiDsQBKEHUiCsANJEHYgCcIOJMGQzTW46qqrivXR0dFi/Y477ijWO50rH+S1EjhzsWUHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSS4nx04y3A/O5AcYQeSIOxAEoQdSIKwA0kQdiAJwg4k0THsti+3vdH2Tttv2P5ONX2Z7YO2X6set/a/XQDd6nhRje1pkqZFxCu2Py/pZUlzJf2tpD9ExMMTXhgX1QB91+6imomMz35Y0uHq9Tu2d0qaXm97APrtM31nt32lpFmStlSTlth+3fZTti9uM89i29tsb+upUwA9mfC18bY/J+k/JP0gItbanirpbUkhaYXGdvX/ocNnsBsP9Fm73fgJhd32JEm/lrQ+In7con6lpF9HxF92+BzCDvRZ1zfC2LakJyXtHB/06sDdKfMk7ei1SQD9M5Gj8V+W9J+Stks6Nb7v9yQtlHStxnbj90r6VnUwr/RZbNmBPutpN74uhB3oP+5nB5Ij7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJNHxBydr9rak/x33/tJq2jAa1t6GtS+J3rpVZ29XtCsM9H72Ty3c3hYRsxtroGBYexvWviR669agemM3HkiCsANJNB32kYaXXzKsvQ1rXxK9dWsgvTX6nR3A4DS9ZQcwIIQdSKKRsNu+xfbvbO+2vbSJHtqxvdf29moY6kbHp6vG0Dtie8e4aZfYfsH2m9VzyzH2GuptKIbxLgwz3ui6a3r484F/Z7d9rqRdkr4m6YCkrZIWRsRvB9pIG7b3SpodEY1fgGH7Jkl/kPSvp4bWsv0vko5FxA+r/ygvjoh/GpLelukzDuPdp97aDTP+92pw3dU5/Hk3mtiyXydpd0TsiYg/Svq5pDkN9DH0IuJFScdOmzxH0srq9UqN/WMZuDa9DYWIOBwRr1Sv35F0apjxRtddoa+BaCLs0yXtH/f+gIZrvPeQ9BvbL9te3HQzLUw9NcxW9Tyl4X5O13EY70E6bZjxoVl33Qx/3qsmwt5qaJphOv93Y0T8laS/kfTtancVE/MTSV/S2BiAhyX9qMlmqmHGn5X03Yj4fZO9jNeir4GstybCfkDS5ePef0HSoQb6aCkiDlXPRyT9UmNfO4bJ6KkRdKvnIw338/8iYjQiPo6Ik5J+qgbXXTXM+LOSVkXE2mpy4+uuVV+DWm9NhH2rpJm2v2h7sqRvSFrXQB+fYvui6sCJbF8k6esavqGo10laVL1eJOm5Bnv5hGEZxrvdMONqeN01Pvx5RAz8IelWjR2Rf0vSPzfRQ5u+Zkj67+rxRtO9SXpaY7t1H2lsj+ibkv5U0gZJb1bPlwxRb/+msaG9X9dYsKY11NuXNfbV8HVJr1WPW5ted4W+BrLeuFwWSIIr6IAkCDuQBGEHkiDsQBKEHUiCsANJEHYgif8DmtE6jFxD9HcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap = 'Greys_r'); \n",
    "print('The image below is labeled as a \"{}\".'.format(labels[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build a neural network for this dataset using weight matrices and matrix multiplication. Then we will do it with PyTorch's **nn** module. \n",
    "\n",
    "For this exersice we will build a *Dense* (*fully-connected*) network. In a dense network, wach unit in one layer is connected to each unit in the next layer. In fully-connected networks, the input to each layer must be a one-dimensional vector. However, our images are 28x28 2D tensors, so we need to convert them into 1D vectors. Thinking about sizes, we need to convert the batch of images with shape (64, 1, 28, 28) to a have a shape of (64, 784), where 28x28 = 784. This is typically called flattening, we flattened the 2D images into 1D vectors.\n",
    "\n",
    "Moreover, unlike above where we had only one output unit in the output layer, here we need 10 output units, one for each digit. We want our network to predict the digit shown in an image, so what we'll do is calculate probabilities that the image is of any one digit or class. This ends up being a discrete probability distribution over the classes (digits) that tells us the most likely class for the image. That means we need 10 output units for the 10 classes (digits). \n",
    "\n",
    "    Exercise: Flatten the batch of images images. Then build a multi-layer network with 784 input units, 256 hidden units, and 10 output units using random tensors for the weights and biases. For now, use a sigmoid activation for the hidden layer. Leave the output layer without an activation, we'll add one that gives us a probability distribution next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the activation function (for now stick to sigmoid activation function)\n",
    "\n",
    "def activation(x):\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "# flatten the input\n",
    "inputs = images.view(images.shape[0], -1) \n",
    "# -1 handles the remaning part so we do not need to calculate it. Here we preserve \n",
    "# 64, the number of the observations and flatten the (1,28,28) part to (784) \n",
    "\n",
    "# create weights and biases\n",
    "# from input to hidden layer\n",
    "w1 = torch.randn(784,256)\n",
    "b1 = torch.randn(256)\n",
    "\n",
    "# from hidden to output \n",
    "w2 = torch.randn(256, 10)\n",
    "b2 = torch.randn(10) \n",
    "\n",
    "h = activation( torch.mm(inputs, w1) + b1 ) # the outputs of the neurons in the hidden layer \n",
    "\n",
    "output = torch.mm(h, w2) + b2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do want our network to tell us the probabilities of an image belonging to a class of digit. In other words, we want it to result in a probability distribution. However, this one yields the following "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3595e+01, -1.2531e+01,  2.4790e+00,  1.3937e+01, -2.8479e+00,\n",
       "          3.3568e+00, -1.5519e+01, -4.7815e+00,  1.2554e+01,  6.8631e-01],\n",
       "        [ 1.3518e+00, -1.8978e+01,  1.2984e+00,  1.0550e+01, -9.7840e+00,\n",
       "          3.4710e+00, -1.5515e+01, -4.4512e+00,  1.4810e+00,  3.9936e+00],\n",
       "        [ 2.5064e+00, -3.4271e+00,  5.1454e-01,  1.6956e+01, -3.7287e+00,\n",
       "         -4.1507e+00, -1.1961e+01,  4.1959e+00,  8.1844e+00,  2.5714e+00],\n",
       "        [ 6.0296e+00, -2.4710e+00,  7.0680e+00,  1.3004e+01, -4.5006e+00,\n",
       "         -9.9589e+00, -1.5161e+01, -1.5491e+01,  2.8066e+00,  3.2765e+00],\n",
       "        [ 3.1348e+00, -1.8122e+01,  1.3104e-02,  1.0954e+01, -1.3886e+01,\n",
       "         -4.0207e+00, -1.9049e+01, -8.7065e+00,  6.4909e+00, -3.0754e+00],\n",
       "        [ 1.7263e+01, -9.6313e+00,  6.4363e+00,  1.4939e+00,  3.0854e+00,\n",
       "         -6.6790e+00, -2.1263e+01, -8.2607e+00,  6.8088e-02, -8.5216e+00],\n",
       "        [ 3.3808e+00, -5.9131e+00,  2.6405e+00,  7.0298e+00, -5.5683e+00,\n",
       "         -9.1513e+00, -2.2089e+01,  4.7298e-01, -5.9112e-01, -1.0076e+01],\n",
       "        [ 1.1839e+01, -1.0046e+01,  5.0474e+00,  1.7698e+01, -5.9025e+00,\n",
       "         -4.7125e+00, -1.8465e+01, -1.9651e+00, -6.6988e-01, -3.3034e+00],\n",
       "        [ 7.3254e+00, -9.3779e+00,  6.2463e+00,  1.6696e+01, -4.3820e+00,\n",
       "         -8.5405e+00, -1.1135e+01, -1.2153e+01, -6.5230e-01, -4.4448e+00],\n",
       "        [ 6.6848e+00, -1.3022e+01,  3.1034e+00,  1.5948e+01, -2.3725e+00,\n",
       "         -3.3021e+00, -8.7450e+00, -1.7695e+00, -3.3657e+00, -5.2511e+00],\n",
       "        [-6.8698e+00, -1.0325e+01,  1.4328e+01,  5.3053e+00, -2.8264e+00,\n",
       "          4.0205e+00, -1.1046e+01,  1.8508e+00,  4.8047e+00, -6.6172e+00],\n",
       "        [ 9.7875e+00, -8.4553e+00, -3.1894e+00,  1.2226e+01, -1.4177e+01,\n",
       "         -1.4433e+01, -1.7553e+01, -1.6082e+01,  6.6234e+00, -8.1996e-01],\n",
       "        [ 7.7946e+00, -1.9391e+01, -1.8301e+00,  1.0301e+01, -5.7606e+00,\n",
       "         -6.2625e+00, -1.0949e+01, -1.1185e+01,  5.7640e+00, -2.2208e+00],\n",
       "        [ 1.1832e+01, -1.9351e+01,  2.8164e+00,  1.1035e+01, -9.3666e+00,\n",
       "          4.8650e+00, -9.2311e+00, -9.5416e+00,  5.6148e+00, -7.7807e+00],\n",
       "        [ 4.6564e+00, -5.2536e+00, -1.8279e+00,  7.0143e+00, -5.9245e+00,\n",
       "         -4.9431e+00, -1.4588e+01, -8.3194e+00,  5.4426e+00, -8.4055e+00],\n",
       "        [ 5.0801e+00, -1.0452e+01, -2.7269e-01,  4.6606e+00, -1.8506e+00,\n",
       "         -9.3759e+00, -1.9079e+01, -5.4077e+00,  7.0210e+00, -3.3934e+00],\n",
       "        [ 5.1159e+00, -1.2052e+01,  5.7620e+00,  1.3746e+01,  4.7177e-01,\n",
       "         -1.2057e+01, -1.7034e+01, -9.0661e+00,  4.6327e+00,  3.3695e+00],\n",
       "        [ 2.3023e-01, -1.2000e+01,  1.0957e+01,  9.9704e+00,  1.0994e+01,\n",
       "          6.3592e+00, -3.0194e+01, -1.2713e+01,  1.1149e+01,  1.1651e+00],\n",
       "        [ 6.2403e+00, -9.1439e+00,  1.9580e+00,  6.5573e+00, -2.6690e+00,\n",
       "         -9.9419e+00, -1.1969e+01, -4.9183e+00,  5.8203e+00, -2.9930e+00],\n",
       "        [ 1.1226e+01, -1.4530e+01,  3.6444e+00,  1.7751e+01,  1.9163e+00,\n",
       "         -2.6541e+00, -1.7133e+01, -9.5981e+00,  2.0635e+00, -9.1832e-01],\n",
       "        [ 6.0773e+00, -1.6654e+01,  5.4309e+00,  1.0434e+01, -3.6344e+00,\n",
       "         -2.4474e+00, -1.1485e+01, -1.0948e+01, -5.3523e+00, -1.8449e+00],\n",
       "        [ 1.2406e+01, -8.2517e+00,  1.0475e+01,  1.2336e+01, -2.0154e+00,\n",
       "         -4.4447e+00, -7.3694e+00, -1.1082e+01,  3.4083e+00, -4.3070e+00],\n",
       "        [ 1.2098e+01, -4.5978e+00,  1.4916e+00,  1.3530e+01,  1.9748e+00,\n",
       "         -6.1631e+00, -1.6937e+01, -4.7065e+00,  3.5670e+00, -3.6835e+00],\n",
       "        [ 5.7035e+00, -1.0581e+01,  6.1174e+00,  6.5932e+00,  1.1544e+00,\n",
       "         -6.7937e+00, -1.2502e+01, -1.0972e+01,  6.2357e+00, -2.0974e+00],\n",
       "        [ 4.3101e+00, -1.8932e+01,  5.6826e+00,  1.4706e+01, -3.4333e+00,\n",
       "         -2.0674e+00, -8.0683e+00, -1.0050e+01,  4.0351e+00, -2.6737e+00],\n",
       "        [ 2.9238e+00, -1.5319e+01,  5.1429e+00,  7.8352e+00, -1.7231e+00,\n",
       "          2.0433e+00, -1.8537e+01, -3.0838e+00,  7.0764e+00, -8.1760e+00],\n",
       "        [ 7.3557e+00, -4.4779e+00, -2.0694e+00,  6.1438e+00,  4.6243e-01,\n",
       "         -1.4583e+00, -1.8249e+01, -3.3835e+00,  1.6977e+00, -5.8096e+00],\n",
       "        [ 1.1009e+01, -6.6134e+00,  2.4753e+00,  3.2349e+00, -9.2815e+00,\n",
       "         -1.3688e+01, -8.3104e+00, -4.7813e+00,  1.1521e+01, -5.5875e+00],\n",
       "        [ 4.7351e+00, -1.0993e+00, -7.1289e-01,  9.7234e+00, -2.6531e+00,\n",
       "          5.3260e+00, -1.0765e+01,  7.6304e+00,  5.4581e+00,  9.2782e+00],\n",
       "        [ 1.3744e+01, -1.3031e+00,  6.2154e+00,  3.3727e+00,  4.6529e+00,\n",
       "         -7.1270e+00, -1.0767e+01, -6.2528e+00,  4.3861e+00, -3.6675e+00],\n",
       "        [ 5.6750e+00, -6.3112e+00,  9.2548e+00,  1.6000e+01, -6.9723e+00,\n",
       "         -1.7563e+01, -1.7369e+01, -7.7138e+00,  2.5101e+00, -5.8669e+00],\n",
       "        [ 4.2636e+00, -6.4773e+00,  3.1750e+00,  8.0801e+00,  3.3743e+00,\n",
       "         -2.2023e+00, -1.5846e+01, -1.1576e+00,  4.4838e+00, -3.8818e+00],\n",
       "        [ 7.8788e+00, -1.1846e+01, -1.8020e-01,  7.1561e+00, -6.5653e+00,\n",
       "         -4.3108e+00, -1.7580e+01, -5.1042e+00, -1.2999e+00, -8.6675e+00],\n",
       "        [ 7.0706e+00, -1.5711e+01,  1.1334e+01,  1.2834e+01,  4.3635e+00,\n",
       "         -4.6220e+00, -1.6004e+01, -1.9389e+00, -9.2064e+00,  2.3683e+00],\n",
       "        [ 1.2956e+01, -1.1349e+01, -6.7670e+00,  5.8156e+00, -6.4252e-01,\n",
       "         -1.5514e+00, -1.7086e+01, -3.2615e+00,  9.6267e+00, -3.2495e+00],\n",
       "        [ 3.7628e+00, -1.6327e+01,  5.3037e+00,  7.1422e+00, -3.5236e+00,\n",
       "         -1.1559e+00, -2.2056e+01,  1.1661e-01,  8.5620e+00, -5.1801e+00],\n",
       "        [ 7.5577e+00, -1.8088e+01, -1.2003e+00,  2.5850e+00, -2.8184e+00,\n",
       "         -3.0548e-01, -1.2775e+01, -7.3625e+00,  5.4508e+00, -1.7515e+00],\n",
       "        [ 4.7601e+00, -8.5619e+00,  6.8328e+00,  9.5011e+00, -1.0137e+01,\n",
       "         -7.7789e+00, -1.2656e+01,  2.4006e+00,  1.2089e+01, -7.8989e+00],\n",
       "        [-5.1711e+00, -2.6288e+00,  5.1579e+00,  1.3351e+01,  4.1578e-01,\n",
       "          7.8157e+00, -1.4152e+01,  4.7711e+00,  6.1375e+00, -2.0370e+00],\n",
       "        [ 2.4295e+00, -8.1805e+00, -1.1150e-01,  1.5552e+01, -6.5029e+00,\n",
       "         -1.6825e+00, -2.2126e+01, -7.8010e+00,  8.1580e+00, -8.5453e+00],\n",
       "        [ 6.9392e+00, -6.4701e+00,  9.7416e+00,  1.7175e+01, -6.1411e+00,\n",
       "          2.4664e+00, -1.5251e+01, -9.5533e+00,  8.3158e+00, -1.0466e+01],\n",
       "        [-2.6378e-01, -2.2304e+01,  8.5236e+00,  8.3541e+00, -1.0681e+01,\n",
       "          1.1842e+00, -2.8470e+01, -8.3971e+00,  2.5590e+00, -1.1284e+01],\n",
       "        [ 4.8340e+00, -1.2187e+01,  1.2466e+01,  7.6387e+00,  2.8049e+00,\n",
       "          7.9644e+00, -1.0910e+01,  1.3073e+00,  9.7938e+00, -3.4767e+00],\n",
       "        [ 7.7495e+00, -2.1081e+01,  4.0515e+00,  1.2069e+01, -6.9521e+00,\n",
       "         -6.0333e+00, -1.0204e+01, -1.5602e+01,  2.2925e+00, -4.0078e+00],\n",
       "        [ 4.8702e+00, -8.4442e+00,  3.6824e-01,  8.9334e+00, -6.0135e+00,\n",
       "         -1.4072e+01, -1.7323e+01, -7.3358e+00,  3.2060e+00, -5.8498e-01],\n",
       "        [ 6.9408e+00, -1.3596e+01,  2.0886e+00,  6.6958e-01, -4.3536e+00,\n",
       "          9.5661e+00, -1.9518e+01, -1.4322e+01, -1.1606e+00, -1.1484e+01],\n",
       "        [ 8.7369e-01, -1.3728e+01,  8.5338e+00, -1.2300e+01,  3.0072e+00,\n",
       "         -2.7679e+00, -1.9824e+01, -7.6287e+00,  6.4979e+00,  7.5131e+00],\n",
       "        [-3.1162e+00, -1.1773e+01,  9.5479e+00,  8.9770e+00, -9.6397e+00,\n",
       "          1.0399e+01, -1.6451e+01, -5.9475e+00,  9.2379e+00, -7.6815e+00],\n",
       "        [ 1.0980e+01, -6.7141e+00,  2.1009e+00,  4.5349e+00,  1.3897e+00,\n",
       "         -5.1549e+00, -1.0105e+01, -9.0193e+00,  7.1164e+00, -3.3746e+00],\n",
       "        [ 3.9606e+00, -1.0651e+01,  7.1517e+00,  3.8651e+00,  5.9466e+00,\n",
       "         -2.9032e+00, -1.1973e+01, -8.2832e+00,  2.2425e+00,  2.2185e+00],\n",
       "        [ 1.3413e+01, -1.8235e+01,  1.0028e+01,  1.3976e+01,  7.7245e+00,\n",
       "          5.0533e+00, -2.0470e+01, -9.0281e+00,  1.0033e+01, -6.9788e+00],\n",
       "        [ 1.0153e+01, -8.3730e+00, -5.3902e-01,  6.4133e+00, -1.1377e+00,\n",
       "         -1.3258e+00, -6.2424e+00, -3.1105e+00, -3.9758e+00, -7.5973e+00],\n",
       "        [ 5.2982e+00, -3.6796e+00,  5.7183e-01,  8.1489e+00, -3.7333e+00,\n",
       "          8.8362e+00, -1.2581e+01, -7.5061e+00,  6.9627e+00, -1.1502e-01],\n",
       "        [ 7.5973e+00, -5.5135e+00, -3.4447e+00,  9.1161e+00, -8.1121e+00,\n",
       "         -7.4511e+00, -8.1390e+00, -2.5221e+00,  1.5486e+01, -4.5769e-01],\n",
       "        [ 2.4683e+00, -6.3407e+00,  5.4664e-01,  1.6959e+01, -4.6142e+00,\n",
       "         -1.5352e+00, -1.4312e+01,  4.6734e-01,  7.0099e+00,  2.6543e-01],\n",
       "        [ 1.0612e+01, -1.5358e+01,  1.7633e+00,  9.2209e+00, -8.6740e+00,\n",
       "         -2.9901e+00, -1.9368e+01, -3.5967e+00,  3.8054e+00, -2.6944e+00],\n",
       "        [ 4.7229e+00, -3.3106e+00, -2.1716e+00,  1.2296e+01, -6.9265e+00,\n",
       "         -4.0392e+00, -4.3612e+00, -1.5746e+00,  5.4489e+00, -2.5402e+00],\n",
       "        [-3.5941e+00, -1.7815e+01,  9.2105e+00,  1.3282e+01, -8.3320e+00,\n",
       "          4.6237e+00, -1.3607e+01, -3.1310e+00,  6.7544e+00, -6.5984e+00],\n",
       "        [-8.0281e-01, -1.4491e+01,  4.4002e+00,  4.1684e+00,  1.1267e+00,\n",
       "         -4.7112e+00, -1.1494e+01, -6.2544e+00,  1.0960e+01, -1.3048e+01],\n",
       "        [ 6.4576e+00, -1.0514e+01,  9.1052e+00,  1.7388e+01, -1.0688e+01,\n",
       "          1.3769e+00, -9.1465e+00, -2.0561e+01, -9.5312e-01, -3.4762e+00],\n",
       "        [ 1.7963e+01, -1.2974e+01,  4.4129e+00,  4.1111e+00, -6.3364e+00,\n",
       "         -1.1283e+01, -1.6137e+01, -1.2335e+01,  1.3455e+01,  1.3273e+00],\n",
       "        [ 9.1669e+00, -6.5850e+00,  2.1949e+00,  6.4598e+00, -1.2373e+00,\n",
       "         -7.1011e+00, -2.2499e+01, -2.5499e+00,  2.8870e+00, -1.0954e+01],\n",
       "        [-8.1187e+00, -2.4790e+01,  4.4252e+00,  2.0289e+01,  5.7985e-01,\n",
       "          5.9707e+00, -1.4743e+01, -1.1452e+01,  1.0294e+00,  2.5902e+00],\n",
       "        [ 1.1656e+01, -6.8998e+00,  1.9502e+00,  1.3811e+01, -6.8991e+00,\n",
       "         -7.8634e+00, -1.6939e+01, -6.4302e+00,  1.2838e-01, -3.7628e+00]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we want to convert these values into a probability distribution. For this we use [softmax](https://en.wikipedia.org/wiki/Softmax_function) function.  It looks like\n",
    "$$ \\sigma(x_i) = \\frac{e^{x_i}}{\\sum_{k} e^{x_k}}$$ \n",
    "\n",
    "    Exercise: Implement a function softmax that performs the softmax calculation and returns probability distributions for each example in the batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    '''\n",
    "    Note that you'll need to pay attention to the shapes when doing this. If you have a tensor a with \n",
    "    shape (64, 10) and a tensor b with shape (64,), doing a/b will give you an error because PyTorch \n",
    "    will try to do the division across the columns (called broadcasting) but you'll get a size mismatch. \n",
    "    The way to think about this is for each of the 64 examples, you only want to divide by one value, \n",
    "    the sum in the denominator. So you need b to have a shape of (64, 1). This way PyTorch will divide \n",
    "    the 10 values in each row of a by the one value in each row of b. Pay attention to how you take \n",
    "    the sum as well. You'll need to define the dim keyword in torch.sum. Setting dim=0 takes the sum \n",
    "    across the rows while dim=1 takes the sum across the columns.\n",
    "    '''\n",
    "    return torch.exp(x) / torch.sum(torch.exp(x), dim=1).view(-1, 1)\n",
    "\n",
    "output_probs = softmax(output) \n",
    "# Check that it has the sahpe (64, 10)\n",
    "print(output_probs.shape)\n",
    "# Check that the probs add up to 1\n",
    "print(output_probs.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='building_networks_with_pytorch'></a>\n",
    "## 5. Building networks with PyTorch from scratch\n",
    "\n",
    "Now we will build the same neural network with PyTorch's **nn** module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        \n",
    "        # We have one hidden layer and one output layer.\n",
    "        # hidden layer takes 784 inputs and outputs 256\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        # output layer takes these 256 inputs and results in 10 \n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "        # Define sigmoid activation and softmax output \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # At the end of the network, we want probability distributions, so we use softmax \n",
    "        self.softmax = nn.Softmax(dim=1)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The following is from the lecture note) \n",
    "\n",
    "Let's go through this bit by bit.\n",
    "\n",
    "```python\n",
    "class Network(nn.Module):\n",
    "```\n",
    "\n",
    "Here we're inheriting from `nn.Module`. Combined with `super().__init__()` this creates a class that tracks the architecture and provides a lot of useful methods and attributes. It is mandatory to inherit from `nn.Module` when you're creating a class for your network. The name of the class itself can be anything.\n",
    "\n",
    "```python\n",
    "self.hidden = nn.Linear(784, 256)\n",
    "```\n",
    "\n",
    "This line creates a module for a linear transformation, $x\\mathbf{W} + b$, with 784 inputs and 256 outputs and assigns it to `self.hidden`. The module automatically creates the weight and bias tensors which we'll use in the `forward` method. You can access the weight and bias tensors once the network (`net`) is created with `net.hidden.weight` and `net.hidden.bias`.\n",
    "\n",
    "```python\n",
    "self.output = nn.Linear(256, 10)\n",
    "```\n",
    "\n",
    "Similarly, this creates another linear transformation with 256 inputs and 10 outputs.\n",
    "\n",
    "```python\n",
    "self.sigmoid = nn.Sigmoid()\n",
    "self.softmax = nn.Softmax(dim=1)\n",
    "```\n",
    "\n",
    "Here I defined operations for the sigmoid activation and softmax output. Setting `dim=1` in `nn.Softmax(dim=1)` calculates softmax across the columns.\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "```\n",
    "\n",
    "PyTorch networks created with `nn.Module` must have a `forward` method defined. It takes in a tensor `x` and passes it through the operations you defined in the `__init__` method.\n",
    "\n",
    "```python\n",
    "x = self.hidden(x)\n",
    "x = self.sigmoid(x)\n",
    "x = self.output(x)\n",
    "x = self.softmax(x)\n",
    "```\n",
    "\n",
    "Here the input tensor `x` is passed through each operation a reassigned to `x`. We can see that the input tensor goes through the hidden layer, then a sigmoid function, then the output layer, and finally the softmax function. It doesn't matter what you name the variables here, as long as the inputs and outputs of the operations match the network architecture you want to build. The order in which you define things in the `__init__` method doesn't matter, but you'll need to sequence the operations correctly in the `forward` method.\n",
    "\n",
    "Now we can create a `Network` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the network and look at it's representation\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clean the code further by using `torch.nn.functional` module to combine layers with the activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        # input to hidden : 784 to 256\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        # hidden to output : 256 to 10\n",
    "        self.output = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # hidden layer with sigmoid activation function\n",
    "        x = F.sigmoid(self.hidden(x))\n",
    "        # ouput layer with sogtmax activation function\n",
    "        x = F.softmax(self.output(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='activation_functions'></a>\n",
    "### 5.A. Activation Functions \n",
    "\n",
    "A function can pass as an activation functions as long as it is not linear. Some common activation functions are Sigmoid, ReLU (rectified linear unit), and Tanh (hyperbolic tangent). \n",
    "\n",
    "In practice, the ReLU is used almost exclusively as the activation function for hidden layers, mainly because it is computationally light. \n",
    "\n",
    "    Exercise: Create a network with 784 input units, a hidden layer with 128 units and a ReLU activation, then a hidden layer with 64 units and a ReLU activation, and finally an output layer with a softmax activation. You can use  ReLU activation with the nn.ReLU module or F.relu function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # from input to first hidden: 784 to 128\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        # from first hidden to second hidden: 128 to 64 \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        # from second hidden to output: 64  to 10\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim = 1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Network()\n",
    "print(model)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='initializing_weights_and_biases'></a>\n",
    "### 5.B. Initializing weights and biases\n",
    "\n",
    "Even though `nn` module automatically initializes the weight, it is possible to customize the initialization. Weights and biases are tensors attached to the layer we define. We can get them with `model.fc1.weight`. You can customize bias and weights by `model.fc1.bias.data.fill_(0)` and `model.fc1.weight.data.normal_(std=0.01)`, for instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0167, -0.0088, -0.0015,  ..., -0.0038,  0.0165,  0.0287],\n",
      "        [ 0.0120, -0.0334,  0.0285,  ...,  0.0128,  0.0206,  0.0316],\n",
      "        [ 0.0028, -0.0170,  0.0043,  ...,  0.0135, -0.0126,  0.0049],\n",
      "        ...,\n",
      "        [-0.0030,  0.0010,  0.0116,  ..., -0.0191, -0.0356, -0.0038],\n",
      "        [ 0.0075,  0.0167, -0.0202,  ...,  0.0094, -0.0067,  0.0044],\n",
      "        [ 0.0092, -0.0278,  0.0349,  ...,  0.0299,  0.0196, -0.0037]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0261,  0.0135, -0.0252,  0.0028, -0.0346, -0.0167, -0.0181,  0.0192,\n",
      "        -0.0162,  0.0156, -0.0127,  0.0289, -0.0123, -0.0328, -0.0063, -0.0131,\n",
      "        -0.0334, -0.0344, -0.0080, -0.0080, -0.0296, -0.0007,  0.0103, -0.0027,\n",
      "        -0.0057,  0.0267,  0.0118, -0.0029, -0.0229,  0.0132,  0.0070,  0.0011,\n",
      "         0.0192,  0.0342, -0.0333,  0.0197,  0.0153,  0.0090,  0.0188, -0.0305,\n",
      "        -0.0039,  0.0084, -0.0124,  0.0297, -0.0283,  0.0132,  0.0203,  0.0325,\n",
      "         0.0129,  0.0013,  0.0282, -0.0348,  0.0161, -0.0333, -0.0010, -0.0250,\n",
      "         0.0107,  0.0040,  0.0173, -0.0081, -0.0349,  0.0080, -0.0157, -0.0353,\n",
      "        -0.0165, -0.0184,  0.0020, -0.0126, -0.0084,  0.0191, -0.0202, -0.0061,\n",
      "         0.0252, -0.0320,  0.0314,  0.0291,  0.0242,  0.0037,  0.0227, -0.0167,\n",
      "         0.0331, -0.0123,  0.0281,  0.0317,  0.0316, -0.0323, -0.0254, -0.0046,\n",
      "         0.0271, -0.0263,  0.0236, -0.0103, -0.0159,  0.0282,  0.0298,  0.0100,\n",
      "         0.0188,  0.0192, -0.0218, -0.0049,  0.0290,  0.0103,  0.0030,  0.0254,\n",
      "        -0.0235,  0.0065,  0.0324, -0.0195, -0.0234,  0.0188,  0.0323,  0.0133,\n",
      "        -0.0268,  0.0244, -0.0028, -0.0096, -0.0289, -0.0271, -0.0229, -0.0188,\n",
      "         0.0014,  0.0091,  0.0123, -0.0346,  0.0121,  0.0134, -0.0013,  0.0010],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc1.weight)\n",
    "print(model.fc1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='forward_pass'></a>\n",
    "### 5.C. Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a built a network, let's see what happens when we pass an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the iterator, get some other set of data \n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# remember, we use a flattened layer as the input layer. \n",
    "# So we must resize the images. \n",
    "images.resize_(64, 1, 784) \n",
    "\n",
    "# let's forward pass an image from out batch of 64\n",
    "prob_dist = model.forward(images[0,:]) \n",
    "\n",
    "img = images[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have chosen the first image in our batch. below we plot this image along with the probability distribution that our network creates on possible outcomes by one forward pass. The function below, `image_and_probdistn()`, draws these two images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "def image_and_probdistn(image, prob_dist):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.imshow(img.view(1, 28, 28).numpy().squeeze(), cmap = 'Greys_r')\n",
    "    ax1.set_title('Image')\n",
    "\n",
    "    plt.sca(ax2)\n",
    "    ax2.barh(np.arange(10), np.array(prob_dist.detach().numpy().tolist()[0]), align='center')\n",
    "    ax2.set_xlim(0,1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_title('Class Probability.')\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_and_probdistn(img, prob_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the initial guess of the network is ambiguous. This is because we moved forward once with the initial weights. Later we will back-propogate our mistakes in order to improve our network's guess. \n",
    "\n",
    "<a id='sequential'></a>\n",
    "## 6. Building Networks using `nn.Sequential` \n",
    "\n",
    "Another way of building the Network class is using `nn.Sequential`. We could have built the same network above by the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "images.resize_(64, 1, 784) \n",
    "prob_dist = model.forward(images[0,:]) \n",
    "img = images[0] \n",
    "image_and_probdistn(img, prob_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training'></a>\n",
    "## 7. Model Training\n",
    "\n",
    "In order to optimize the process, we need to have an objective. That is to minimize the **loss function** (also called the cost) defined as \n",
    "$$\n",
    "\\large \\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels. \n",
    "As usual, you may define a different loss function depending on your objective. \n",
    "\n",
    "Having a nice a convex function as our objective function, we can simply use the first order condition, or computationaly, Gradient Descent. Since we have multiple layers in the network when we move from the input to the output, we will be using the chain rule. Back propogation utilize it to update the weights.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is 2.301328\n"
     ]
    }
   ],
   "source": [
    "# Define the same model witout the softmax step. \n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10))\n",
    "\n",
    "# Define the loss \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get the data \n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "# Flatten images \n",
    "images = images.view([images.shape[0], -1])\n",
    "\n",
    "# Forward pass, get the logits\n",
    "logits = model(images) \n",
    "\n",
    "# Calculate the loss \n",
    "loss = criterion(logits, labels) \n",
    "\n",
    "print('The loss is %f' % loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Exercise: Now let;s build a model with the log-softmax output by using `nn.LofSoftMax` or `F.log_softmax`. Then you can get the actual probabilites by taking the exponential `torch.exp(output)`. With a log-softmax output, you want to use the negative log likelihood loss, `nn.NLLLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss is 2.280704\n",
      "tensor(2.2807, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(), \n",
    "                      nn.Linear(128, 64), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1)\n",
    "                     ) \n",
    "# define the loss\n",
    "criterion = nn.NLLLoss() \n",
    "\n",
    "# get the data\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# forward pass, get logits\n",
    "log_probs = model(images)\n",
    "# calculate the loss with the logits and the labels\n",
    "loss = criterion(log_probs, labels)\n",
    "print('The loss is %f' % loss)\n",
    "print(loss) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='autograd'></a>\n",
    "### 7.A. Autograd\n",
    "\n",
    "PyTorch provides a module, `autograd`, for automatically calculating the gradients of tensors. \n",
    "Autograd works by keeping track of operations performed on tensors, then going backwards through those operations, calculating gradients along the way\n",
    "You can turn off gradients for a block of code with the `torch.no_grad()` content:\n",
    "```python\n",
    "x = torch.zeros(1, requires_grad=True)\n",
    ">>> with torch.no_grad():\n",
    "...     y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0276,  0.0181],\n",
      "        [ 0.7817, -1.0574]], requires_grad=True)\n",
      "tensor([[7.6416e-04, 3.2865e-04],\n",
      "        [6.1103e-01, 1.1181e+00]], grad_fn=<PowBackward0>)\n",
      "<PowBackward0 object at 0x00000218D8CB3DC8>\n",
      "tensor(0.4326, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# create a 2x2 tensor with gradient method on\n",
    "x = torch.randn(2,2, requires_grad = True)\n",
    "print(x) \n",
    "\n",
    "# square it and assign it to a new var y\n",
    "y = x**2\n",
    "print(y) \n",
    "\n",
    "print(y.grad_fn) \n",
    "\n",
    "z = y.mean() \n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the gradients, you need to run the `.backward` method on a Variable, `z` for example. This will calculate the gradient for `z` with respect to `x`\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([[ 0.0138,  0.0091],\n",
      "        [ 0.3908, -0.5287]])\n",
      "tensor([[ 0.0138,  0.0091],\n",
      "        [ 0.3908, -0.5287]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# without z.backward() x.grad is none\n",
    "print(x.grad)\n",
    "\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='loss_autograd'></a>\n",
    "### 7.B. Loss and Autograd Together \n",
    "\n",
    "PyTroch creates a network with all of the parameters being initialized with `requires_grad = True`. As a result, we can calculate `loss.backward()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(), \n",
    "                      nn.Linear(128, 64), \n",
    "                      nn.ReLU(), \n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1)\n",
    "                     ) \n",
    "# define the loss\n",
    "criterion = nn.NLLLoss() \n",
    "\n",
    "# get the data\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# forward pass, get logits\n",
    "log_probs = model(images)\n",
    "# calculate the loss with the logits and the labels\n",
    "loss = criterion(log_probs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[-0.0003, -0.0003, -0.0003,  ..., -0.0003, -0.0003, -0.0003],\n",
      "        [-0.0019, -0.0019, -0.0019,  ..., -0.0019, -0.0019, -0.0019],\n",
      "        [-0.0014, -0.0014, -0.0014,  ..., -0.0014, -0.0014, -0.0014],\n",
      "        ...,\n",
      "        [ 0.0005,  0.0005,  0.0005,  ...,  0.0005,  0.0005,  0.0005],\n",
      "        [ 0.0006,  0.0006,  0.0006,  ...,  0.0006,  0.0006,  0.0006],\n",
      "        [ 0.0004,  0.0004,  0.0004,  ...,  0.0004,  0.0004,  0.0004]])\n"
     ]
    }
   ],
   "source": [
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training_network'></a>\n",
    "### 7.C. Training the network\n",
    "\n",
    "Now we can train our network by optimizing the weights using the gradients for gradient descent.  To do this, we will use optimizers, which comes from PyTorch's `optim.SGD` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim \n",
    "\n",
    "# optimizers require the parameters to optimize and learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to use all the individual parts so it's time to see how they work together. Let's consider just one learning step before looping through all the data. The general process with PyTorch:\n",
    "\n",
    "* Make a forward pass through the network \n",
    "* Use the network output to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "Below we go through one training step and print out the weights and gradients so you can see how it changes. \n",
    "\n",
    "    Note that there is a line of code `optimizer.zero_grad()`. When you do multiple backwards passes with the same parameters, the gradients are accumulated. This means that you need to zero the gradients on each training pass or you'll retain gradients from previous training batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[ 0.0020, -0.0033,  0.0261,  ...,  0.0103, -0.0185,  0.0270],\n",
      "        [ 0.0237, -0.0303, -0.0017,  ..., -0.0344,  0.0017, -0.0323],\n",
      "        [ 0.0239,  0.0036, -0.0093,  ...,  0.0320, -0.0135, -0.0229],\n",
      "        ...,\n",
      "        [-0.0350,  0.0004,  0.0184,  ...,  0.0073, -0.0171, -0.0142],\n",
      "        [-0.0081, -0.0022,  0.0276,  ..., -0.0188, -0.0340, -0.0062],\n",
      "        [ 0.0259, -0.0312, -0.0056,  ..., -0.0129, -0.0133,  0.0129]],\n",
      "       requires_grad=True)\n",
      "Gradient - tensor([[ 0.0020,  0.0020,  0.0020,  ...,  0.0020,  0.0020,  0.0020],\n",
      "        [-0.0010, -0.0010, -0.0010,  ..., -0.0010, -0.0010, -0.0010],\n",
      "        [ 0.0029,  0.0029,  0.0029,  ...,  0.0029,  0.0029,  0.0029],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0026, -0.0026, -0.0026,  ..., -0.0026, -0.0026, -0.0026],\n",
      "        [ 0.0066,  0.0066,  0.0066,  ...,  0.0066,  0.0066,  0.0066]])\n",
      "Updated weights -  Parameter containing:\n",
      "tensor([[ 0.0020, -0.0033,  0.0261,  ...,  0.0103, -0.0185,  0.0270],\n",
      "        [ 0.0237, -0.0303, -0.0017,  ..., -0.0344,  0.0017, -0.0323],\n",
      "        [ 0.0239,  0.0036, -0.0093,  ...,  0.0319, -0.0135, -0.0229],\n",
      "        ...,\n",
      "        [-0.0350,  0.0004,  0.0184,  ...,  0.0073, -0.0171, -0.0142],\n",
      "        [-0.0081, -0.0021,  0.0276,  ..., -0.0188, -0.0340, -0.0062],\n",
      "        [ 0.0259, -0.0313, -0.0057,  ..., -0.0130, -0.0133,  0.0129]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model[0].weight)\n",
    "\n",
    "# get the data\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "# flatten it\n",
    "images.resize_(64,784)\n",
    "\n",
    "# clear the gradients in order to avoid the accumulation\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# forward pass, backward pass, update weights \n",
    "output = model(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradient -', model[0].weight.grad)\n",
    "\n",
    "# Take an update step and few the new weights\n",
    "optimizer.step()\n",
    "print('Updated weights - ', model[0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='training_forreal'></a>\n",
    "## 8. Training For Real\n",
    "\n",
    "Now let's loop over the optimization process to update our weights in order to minimize the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1 - Training loss: 1.916575224033551\n",
      " Epoch 2 - Training loss: 0.8912317223195583\n",
      " Epoch 3 - Training loss: 0.542008754302825\n",
      " Epoch 4 - Training loss: 0.4363247998424176\n",
      " Epoch 5 - Training loss: 0.38851464018702253\n",
      " Epoch 6 - Training loss: 0.36104277241776495\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                     nn.ReLU(), \n",
    "                     nn.Linear(128,64),\n",
    "                     nn.ReLU(), \n",
    "                     nn.Linear(64,10), \n",
    "                     nn.LogSoftmax(dim = 1)\n",
    "                     )\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.003)\n",
    "\n",
    "epochs = 6\n",
    "for e in range(epochs):\n",
    "    # accumulate the losses as trainloader loads data \n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # flatten the images\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        # do not forget to skip this step\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass, calculate the loss, backward pass, take a step\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # addup the losses\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\" Epoch {e+1} - Training loss: {running_loss/len(trainloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained the model, it is time to check its predcitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(testloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "with torch.no_grad():\n",
    "    log_probs = model(img)\n",
    "logits =torch.exp(log_probs)\n",
    "    \n",
    "\n",
    "image_and_probdistn(img, logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have build a wonderful neural network that can accurately predict the digits in mnist dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
